{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d2b222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'UAFTC'...\n",
      "remote: Enumerating objects: 89, done.\u001b[K\n",
      "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
      "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
      "remote: Total 89 (delta 37), reused 66 (delta 18), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (89/89), 19.24 MiB | 20.61 MiB/s, done.\n",
      "Resolving deltas: 100% (37/37), done.\n",
      "/content/UAFTC/UAFTC\n",
      "appendix_2020.pdf\t\t\t\t      data_processor.py\n",
      "args\t\t\t\t\t\t      derivatives.pdf\n",
      "attention_score_binary_classification.ipynb\t      dynamic_lstm.py\n",
      "attn_model.py\t\t\t\t\t      helper.py\n",
      "attn_neural_classification_interpret_synthetic.ipynb  imgs\n",
      "data\t\t\t\t\t\t      README.md\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/richardsun-voyager/UAFTC.git\n",
    "%cd UAFTC\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0683d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p data/custom\n",
    "!mkdir -p results/sst\n",
    "!mkdir -p results/custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916d3285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined with dropout (p=0.5)\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Attention Classifier Model WITH DROPOUT\n",
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, lambda_scale, dropout=0.5):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.V = nn.Parameter(torch.randn(embed_dim))  # Context vector\n",
    "        self.W = nn.Parameter(torch.randn(embed_dim))  # Linear layer weight\n",
    "        self.lambda_scale = lambda_scale\n",
    "\n",
    "        # Add dropout layer (as in the paper)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize with uniform distribution [-0.1, 0.1]\n",
    "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.V, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.W, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        # x: [batch_size, seq_len]\n",
    "        embeds = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # Apply dropout to embeddings (during training)\n",
    "        embeds = self.dropout(embeds)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(embeds, self.V) / self.lambda_scale\n",
    "\n",
    "        # Compute attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # Weighted sum\n",
    "        context = torch.sum(embeds * attention_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "        # Compute polarity score\n",
    "        output = torch.matmul(context, self.W)\n",
    "\n",
    "        if return_attention:\n",
    "            token_polarity = torch.matmul(embeds, self.W)\n",
    "            return output, attention_weights, attention_scores, token_polarity\n",
    "\n",
    "        return output\n",
    "\n",
    "print(\"Model defined with dropout (p=0.5)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
