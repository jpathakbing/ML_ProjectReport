{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d2b222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'UAFTC'...\n",
      "remote: Enumerating objects: 89, done.\u001b[K\n",
      "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
      "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
      "remote: Total 89 (delta 37), reused 66 (delta 18), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (89/89), 19.24 MiB | 20.61 MiB/s, done.\n",
      "Resolving deltas: 100% (37/37), done.\n",
      "/content/UAFTC/UAFTC\n",
      "appendix_2020.pdf\t\t\t\t      data_processor.py\n",
      "args\t\t\t\t\t\t      derivatives.pdf\n",
      "attention_score_binary_classification.ipynb\t      dynamic_lstm.py\n",
      "attn_model.py\t\t\t\t\t      helper.py\n",
      "attn_neural_classification_interpret_synthetic.ipynb  imgs\n",
      "data\t\t\t\t\t\t      README.md\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/richardsun-voyager/UAFTC.git\n",
    "%cd UAFTC\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0683d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p data/custom\n",
    "!mkdir -p results/sst\n",
    "!mkdir -p results/custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916d3285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined with dropout (p=0.5)\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Attention Classifier Model WITH DROPOUT\n",
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, lambda_scale, dropout=0.5):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.V = nn.Parameter(torch.randn(embed_dim))  # Context vector\n",
    "        self.W = nn.Parameter(torch.randn(embed_dim))  # Linear layer weight\n",
    "        self.lambda_scale = lambda_scale\n",
    "\n",
    "        # Add dropout layer (as in the paper)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize with uniform distribution [-0.1, 0.1]\n",
    "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.V, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.W, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        # x: [batch_size, seq_len]\n",
    "        embeds = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # Apply dropout to embeddings (during training)\n",
    "        embeds = self.dropout(embeds)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(embeds, self.V) / self.lambda_scale\n",
    "\n",
    "        # Compute attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # Weighted sum\n",
    "        context = torch.sum(embeds * attention_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "        # Compute polarity score\n",
    "        output = torch.matmul(context, self.W)\n",
    "\n",
    "        if return_attention:\n",
    "            token_polarity = torch.matmul(embeds, self.W)\n",
    "            return output, attention_weights, attention_scores, token_polarity\n",
    "\n",
    "        return output\n",
    "\n",
    "print(\"Model defined with dropout (p=0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c310271",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c7a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data, min_freq=1):\n",
    "    \"\"\"Build vocabulary from data\"\"\"\n",
    "    word_counts = Counter()\n",
    "    for text, _ in data:\n",
    "        words = text.split()\n",
    "        word_counts.update(words)\n",
    "\n",
    "    # Use explicit PAD and UNK tokens\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    idx = 2\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    return vocab\n",
    "\n",
    "def text_to_indices(text, vocab, max_len=50):\n",
    "    \"\"\"Convert text to indices\"\"\"\n",
    "    words = text.split()[:max_len]\n",
    "    indices = [vocab.get(word, vocab['<UNK>']) for word in words]\n",
    "    # Pad to fixed length\n",
    "    while len(indices) < max_len:\n",
    "        indices.append(vocab['<PAD>'])\n",
    "    return indices\n",
    "\n",
    "def train_model(model, train_data, dev_data, vocab, num_epochs, device, verbose=False):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "\n",
    "    best_dev_acc = 0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        random.shuffle(train_data)\n",
    "\n",
    "        for text, label in train_data:\n",
    "            indices = text_to_indices(text, vocab)\n",
    "            x = torch.LongTensor([indices]).to(device)\n",
    "            y = torch.FloatTensor([label]).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "\n",
    "            y_scaled = 2 * y - 1\n",
    "            loss = criterion(output * y_scaled, torch.ones_like(output))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pred = (torch.sigmoid(output) > 0.5).float()\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += 1\n",
    "\n",
    "        train_acc = correct / total\n",
    "        dev_acc = evaluate_model(model, dev_data, vocab, device)\n",
    "\n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss/total:.4f}, Train: {train_acc:.4f}, Dev: {dev_acc:.4f}\")\n",
    "\n",
    "        if dev_acc > best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    return best_dev_acc\n",
    "\n",
    "def evaluate_model(model, data, vocab, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, label in data:\n",
    "            indices = text_to_indices(text, vocab)\n",
    "            x = torch.LongTensor([indices]).to(device)\n",
    "            y = torch.FloatTensor([label]).to(device)\n",
    "\n",
    "            output = model(x)\n",
    "            pred = (torch.sigmoid(output) > 0.5).float()\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += 1\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
